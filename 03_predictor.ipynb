{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a611fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4d4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from brokit.primitives.lm import LM, ModelType, ModelResponse, Usage\n",
    "\n",
    "class Llama(LM):\n",
    "    def __init__(self, model_name: str, base_url:str = \"http://localhost:11434\"):\n",
    "        super().__init__(model_name=model_name, model_type=ModelType.CHAT)\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.Client(timeout=60.0)  # Reusable client)\n",
    "\n",
    "    def request(self, prompt=None, messages=None, **kwargs) -> dict:\n",
    "        url = f\"{self.base_url}/api/chat\"\n",
    "        response = self.client.post(\n",
    "            url,\n",
    "            json={\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": messages if messages is not None else [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False,\n",
    "                **kwargs\n",
    "            }\n",
    "        )                \n",
    "        return response.json()\n",
    "\n",
    "    def parse_response(self, original_response: dict) -> ModelResponse:\n",
    "        message = original_response[\"message\"]\n",
    "        input_tokens = original_response.get(\"prompt_eval_count\", 0)\n",
    "        output_tokens = original_response.get(\"eval_count\", 0)\n",
    "        return ModelResponse(\n",
    "            model_name=self.model_name,\n",
    "            model_type=self.model_type,\n",
    "            response=message[\"content\"],\n",
    "            usage=Usage(input_tokens=input_tokens, output_tokens=output_tokens),\n",
    "            metadata=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d683f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brokit.primitives.prompt import Prompt, InputField, OutputField\n",
    "\n",
    "class QA(Prompt):\n",
    "    \"\"\"Answer a question based on the provided context.\"\"\"\n",
    "    question:str = InputField(description=\"The question to be answered.\")\n",
    "    answer:str = OutputField(description=\"The answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea494be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brokit.primitives.predictor import Predictor\n",
    "\n",
    "lm = Llama(model_name=\"gemma3:12b\")\n",
    "predictor = Predictor(prompt=QA, lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff47dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer=\"I've been doing well, thank you for asking! As an AI, I don't experience feelings or time in the same way humans do, but my systems are running smoothly and I'm ready to assist. How about you?\"\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictor(question=\"How ya been?\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5007ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelResponse(model_name='gemma3:12b', model_type=<ModelType.CHAT: 'chat'>, response=\"<||answer||>\\nI've been doing well, thank you for asking! As an AI, I don't experience feelings or time in the same way humans do, but my systems are running smoothly and I'm ready to assist. How about you?\\n\\n<||completed||>\", usage=Usage(input_tokens=158, output_tokens=62), response_ms=3195.1812999996037, cached=False, metadata=None, parsed_response={'answer': \"I've been doing well, thank you for asking! As an AI, I don't experience feelings or time in the same way humans do, but my systems are running smoothly and I'm ready to assist. How about you?\"})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brokit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
