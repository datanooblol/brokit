{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846c7eac",
   "metadata": {},
   "source": [
    "# Create LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d8eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from typing import List, Optional\n",
    "import brokit as bk\n",
    "class Llama(bk.LM):\n",
    "    def __init__(self, model_name: str, base_url:str = \"http://localhost:11434\", temperature:float=0.0, top_p:float=1.0, seed:int=55, **kwargs):\n",
    "        super().__init__(model_name=model_name, model_type=bk.ModelType.CHAT)\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.Client(timeout=60.0)  # Reusable client\n",
    "        self.model_params = {\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"seed\": seed,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "    def request(self, prompt:Optional[str]=None, messages:Optional[List[bk.Message]]=None, images:Optional[List[bk.Image]]=None, audios=None, **kwargs) -> dict:\n",
    "        url = f\"{self.base_url}/api/chat\"\n",
    "        params = {**self.model_params, **kwargs}\n",
    "        if messages is not None:\n",
    "            _messages = [msg.to_dict() if isinstance(msg, bk.Message) else msg for msg in messages]\n",
    "        else:\n",
    "            _messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if images:\n",
    "            _messages[-1][\"images\"] = [image.to_base64() for image in images]\n",
    "        request_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": _messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {**params},\n",
    "        }\n",
    "        response = self.client.post(\n",
    "            url,\n",
    "            json=request_params\n",
    "        )                \n",
    "        return response.json()\n",
    "\n",
    "    def parse_response(self, original_response: dict) -> bk.ModelResponse:\n",
    "        message = original_response[\"message\"]\n",
    "        input_tokens = original_response.get(\"prompt_eval_count\", 0)\n",
    "        output_tokens = original_response.get(\"eval_count\", 0)\n",
    "        return bk.ModelResponse(\n",
    "            model_name=self.model_name,\n",
    "            model_type=self.model_type,\n",
    "            response=message[\"content\"],\n",
    "            usage=bk.Usage(input_tokens=input_tokens, output_tokens=output_tokens),\n",
    "            metadata=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2571b5e",
   "metadata": {},
   "source": [
    "# Test LM with normal prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfd6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = Llama(model_name=\"gemma3:12b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7373f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gemma3:12b',\n",
       " 'created_at': '2026-02-07T06:12:15.4328527Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but I'm operating smoothly and ready to assist you. ðŸ˜Š \\n\\nHow are *you* doing today?\"},\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'total_duration': 3314625500,\n",
       " 'load_duration': 2540963000,\n",
       " 'prompt_eval_count': 15,\n",
       " 'prompt_eval_duration': 55795500,\n",
       " 'eval_count': 51,\n",
       " 'eval_duration': 673059900}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.request(prompt=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ea485b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " ModelResponse(model_name='gemma3:12b', model_type=<ModelType.CHAT: 'chat'>, response=\"Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but I'm operating smoothly and ready to help. ðŸ˜Š\\n\\nHow are *you* doing today?\", usage=Usage(input_tokens=15, output_tokens=49), response_ms=916.8049999861978, cached=False, metadata=None, request=None, parsed_response=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama(prompt=\"Hello, how are you?\")\n",
    "response.cached, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5b4c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " ModelResponse(model_name='gemma3:12b', model_type=<ModelType.CHAT: 'chat'>, response=\"Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but I'm operating smoothly and ready to help. ðŸ˜Š\\n\\nHow are *you* doing today?\", usage=Usage(input_tokens=15, output_tokens=49), response_ms=916.8049999861978, cached=True, metadata=None, request=None, parsed_response=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resopnse = llama(prompt=\"Hello, how are you?\")\n",
    "response.cached, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e794098",
   "metadata": {},
   "source": [
    "# Test LM with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac922972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doraemon1 = bk.Image(\"https://aithailand.co.th/wp-content/uploads/2024/09/Doraemon-th_1630640671-1024x635.jpg\")\n",
    "nobita1 = bk.Image(\"https://m.media-amazon.com/images/M/MV5BM2E5MDIwNjktOWQwYS00ODg5LTlhN2MtNzU1OTc3MDA0NjNmXkEyXkFqcGc@._V1_FMjpg_UX1000_.jpg\")\n",
    "doraemon1.to_base64() == nobita1.to_base64()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3980d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of what's in the images:\n",
      "\n",
      "**Image 1:**\n",
      "\n",
      "*   **Character:** It's Doraemon, a popular Japanese manga and anime character. He's a robotic cat from the future.\n",
      "*   **Appearance:** He's blue, has a red nose, big eyes, and a bell around his neck. He's waving.\n",
      "*   **Text:** The word \"DORAEMON\" is written below his image.\n",
      "*   **Background:** A light blue background with a white border.\n",
      "\n",
      "**Image 2:**\n",
      "\n",
      "*   **Character:** This is Nobita Nobi, another character from the Doraemon series. He's the main protagonist.\n",
      "*   **Appearance:** He's a young boy with black hair, glasses, and a slightly mischievous expression.\n",
      "*   **Background:** A blue sky with a green tree and a roof.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to know more about Doraemon or Nobita!\n"
     ]
    }
   ],
   "source": [
    "llama = Llama(model_name=\"gemma3:12b\")\n",
    "response = llama(prompt=\"What's in the image??\", images=[doraemon1, nobita1])\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8013bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Let's identify the characters from the cartoon Doraemon.\n",
      "\n",
      "**Image 1:**\n",
      "\n",
      "*   This is **Doraemon** himself! He's the main character of the series. He's a robotic cat from the future sent back in time to help a young boy named Nobita.\n",
      "\n",
      "**Image 2:**\n",
      "\n",
      "*   This is **Nobita Nobi**. He's the boy that Doraemon is sent to help. He's known for being clumsy and often needing Doraemon's gadgets to get out of trouble.\n",
      "\n",
      "Doraemon is a very popular and beloved Japanese manga and anime series!\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to know more about the characters or the show!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gemma3:12b\" # this model can take more than one image at a time\n",
    "# model_name = \"llama3.2-vision:11b\" # this model can take only one image at a time\n",
    "llama = Llama(model_name=model_name)\n",
    "prompt = \"Who are they from cartoon, Doraemon?\" # if we give some hint, describing images is suddenly an easy task\n",
    "# response = llama(prompt=prompt, images=[nobita1])\n",
    "# response = llama(prompt=prompt, images=[doraemon1])\n",
    "response = llama(prompt=prompt, images=[doraemon1, nobita1])\n",
    "# response = llama(prompt=prompt, images=[nobita1, doraemon1])\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c792d11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61839835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brokit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
