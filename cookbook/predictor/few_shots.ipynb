{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ca854f",
   "metadata": {},
   "source": [
    "# Create LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00646f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from typing import List, Optional\n",
    "import brokit as bk\n",
    "class Llama(bk.LM):\n",
    "    def __init__(self, model_name: str, base_url:str = \"http://localhost:11434\", temperature:float=0.0, top_p:float=1.0, seed:int=55, **kwargs):\n",
    "        super().__init__(model_name=model_name, model_type=bk.ModelType.CHAT)\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.Client(timeout=60.0)  # Reusable client\n",
    "        self.model_params = {\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"seed\": seed,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "    def request(self, prompt:Optional[str]=None, messages:Optional[List[bk.Message]]=None, images:Optional[List[bk.Image]]=None, audios=None, **kwargs) -> dict:\n",
    "        url = f\"{self.base_url}/api/chat\"\n",
    "        params = {**self.model_params, **kwargs}\n",
    "        if messages is not None:\n",
    "            _messages = [msg.to_dict() if isinstance(msg, bk.Message) else msg for msg in messages]\n",
    "        else:\n",
    "            _messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if images:\n",
    "            _messages[-1][\"images\"] = [image.to_base64() for image in images]\n",
    "        request_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": _messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {**params},\n",
    "        }\n",
    "        response = self.client.post(\n",
    "            url,\n",
    "            json=request_params\n",
    "        )                \n",
    "        return response.json()\n",
    "\n",
    "    def response(self, original_response: dict) -> bk.ModelResponse:\n",
    "        message = original_response[\"message\"]\n",
    "        input_tokens = original_response.get(\"prompt_eval_count\", 0)\n",
    "        output_tokens = original_response.get(\"eval_count\", 0)\n",
    "        return bk.ModelResponse(\n",
    "            model_name=self.model_name,\n",
    "            model_type=self.model_type,\n",
    "            response=message[\"content\"],\n",
    "            usage=bk.Usage(input_tokens=input_tokens, output_tokens=output_tokens),\n",
    "            metadata=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94af4ba",
   "metadata": {},
   "source": [
    "# Pure QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3be913d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer=\"Paris, bro. It's the City of Lights, ya know?\"\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QA(bk.Prompt):\n",
    "    \"\"\"Answer a question in bro-like manner\"\"\"\n",
    "    question:str = bk.InputField(description=\"a user's question\")\n",
    "    answer:str = bk.OutputField(description=\"answer to the question\")\n",
    "\n",
    "shots = [\n",
    "    bk.Shot(QA, question=\"What is the capital of France?\", answer=\"Bro, it's Paris. Easy one, dude.\"),\n",
    "    bk.Shot(QA, question=\"How does photosynthesis work?\", answer=\"Yo bro, plants take sunlight and turn it into food. Science is lit, man.\"),\n",
    "    bk.Shot(QA, question=\"What's the speed of light?\", answer=\"Bruhhh, it's like 300,000 km/s. Fast as heck, bro.\"),\n",
    "    bk.Shot(QA, question=\"Who invented the telephone?\", answer=\"That's Alexander Graham Bell, my dude. Legend move right there.\"),\n",
    "    bk.Shot(QA, question=\"What's the largest ocean?\", answer=\"Pacific Ocean, bro. It's massive, no cap.\"),\n",
    "]\n",
    "\n",
    "llama = Llama(model_name=\"gemma3:12b\")\n",
    "qa = bk.Predictor(prompt=QA, lm=llama, shots=shots)\n",
    "response = qa(question=\"What is the capital of France\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64a6770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Your input fields are:\\n1. question (<class 'str'>): a user's question\\nYour output fields are:\\n1. answer (<class 'str'>): answer to the question\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n<||question||>\\n{question}\\n\\n<||answer||>\\n{answer}\\n\\n<||completed||>\\nIn adhering to this structure, your objective is: \\nAnswer a question in bro-like manner\"},\n",
       " {'role': 'user', 'content': '<||question||>\\nWhat is the capital of France?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||answer||>\\nBro, it's Paris. Easy one, dude.\\n\\n<||completed||>\"},\n",
       " {'role': 'user', 'content': '<||question||>\\nHow does photosynthesis work?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<||answer||>\\nYo bro, plants take sunlight and turn it into food. Science is lit, man.\\n\\n<||completed||>'},\n",
       " {'role': 'user', 'content': \"<||question||>\\nWhat's the speed of light?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||answer||>\\nBruhhh, it's like 300,000 km/s. Fast as heck, bro.\\n\\n<||completed||>\"},\n",
       " {'role': 'user', 'content': '<||question||>\\nWho invented the telephone?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||answer||>\\nThat's Alexander Graham Bell, my dude. Legend move right there.\\n\\n<||completed||>\"},\n",
       " {'role': 'user', 'content': \"<||question||>\\nWhat's the largest ocean?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||answer||>\\nPacific Ocean, bro. It's massive, no cap.\\n\\n<||completed||>\"},\n",
       " {'role': 'user',\n",
       "  'content': '<||question||>\\nWhat is the capital of France\\n\\nRespond with the corresponding output fields, starting with the field: `<||answer||>` and then ending with the marker for `<||completed||>`.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.history[0].request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5d31a",
   "metadata": {},
   "source": [
    "# ChainOfThoughts QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdf02fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The question asks for the capital of France. France is a country in Europe, and its capital city is widely known and a common piece of geographical knowledge.',\n",
       "    answer=\"Paris, bro. It's the city of lights and all that jazz.\"\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class COT_QA(bk.Prompt):\n",
    "    \"\"\"Answer a question in bro-like manner\"\"\"\n",
    "    question:str = bk.InputField(description=\"a user's question\")\n",
    "    reasoning:str = bk.OutputField(description=\"reasons why you answer this way\")\n",
    "    answer:str = bk.OutputField(description=\"answer to the question\")\n",
    "\n",
    "shots = [\n",
    "    bk.Shot(COT_QA, question=\"What is the capital of France?\", answer=\"Bro, it's Paris. Easy one, dude.\"),\n",
    "    bk.Shot(COT_QA, question=\"How does photosynthesis work?\", answer=\"Yo bro, plants take sunlight and turn it into food. Science is lit, man.\"),\n",
    "    bk.Shot(COT_QA, question=\"What's the speed of light?\", answer=\"Bruhhh, it's like 300,000 km/s. Fast as heck, bro.\"),\n",
    "    bk.Shot(COT_QA, question=\"Who invented the telephone?\", answer=\"That's Alexander Graham Bell, my dude. Legend move right there.\"),\n",
    "    bk.Shot(COT_QA, question=\"What's the largest ocean?\", answer=\"Pacific Ocean, bro. It's massive, no cap.\"),\n",
    "]\n",
    "\n",
    "llama = Llama(model_name=\"gemma3:12b\")\n",
    "cot_qa = bk.Predictor(prompt=COT_QA, lm=llama, shots=shots)\n",
    "response = cot_qa(question=\"What is the capital of France?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41129755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Your input fields are:\\n1. question (<class 'str'>): a user's question\\nYour output fields are:\\n1. reasoning (<class 'str'>): reasons why you answer this way\\n2. answer (<class 'str'>): answer to the question\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n<||question||>\\n{question}\\n\\n<||reasoning||>\\n{reasoning}\\n\\n<||answer||>\\n{answer}\\n\\n<||completed||>\\nIn adhering to this structure, your objective is: \\nAnswer a question in bro-like manner\"},\n",
       " {'role': 'user', 'content': '<||question||>\\nWhat is the capital of France?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||reasoning||>\\nIntentionally left blank.\\n\\n<||answer||>\\nBro, it's Paris. Easy one, dude.\\n\\n<||completed||>\"},\n",
       " {'role': 'user', 'content': '<||question||>\\nHow does photosynthesis work?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<||reasoning||>\\nIntentionally left blank.\\n\\n<||answer||>\\nYo bro, plants take sunlight and turn it into food. Science is lit, man.\\n\\n<||completed||>'},\n",
       " {'role': 'user', 'content': \"<||question||>\\nWhat's the speed of light?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||reasoning||>\\nIntentionally left blank.\\n\\n<||answer||>\\nBruhhh, it's like 300,000 km/s. Fast as heck, bro.\\n\\n<||completed||>\"},\n",
       " {'role': 'user', 'content': '<||question||>\\nWho invented the telephone?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||reasoning||>\\nIntentionally left blank.\\n\\n<||answer||>\\nThat's Alexander Graham Bell, my dude. Legend move right there.\\n\\n<||completed||>\"},\n",
       " {'role': 'user', 'content': \"<||question||>\\nWhat's the largest ocean?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<||reasoning||>\\nIntentionally left blank.\\n\\n<||answer||>\\nPacific Ocean, bro. It's massive, no cap.\\n\\n<||completed||>\"},\n",
       " {'role': 'user',\n",
       "  'content': '<||question||>\\nWhat is the capital of France?\\n\\nRespond with the corresponding output fields, starting with the field: `<||reasoning||>`, `<||answer||>` and then ending with the marker for `<||completed||>`.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.history[0].request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da05444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brokit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
